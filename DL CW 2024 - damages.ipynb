{"cells":[{"cell_type":"markdown","metadata":{"id":"fE4PJVsod6A3"},"source":["# Damages - Deep Learning Coursework 2024\n","\n","The aim of this coursework will be for you to design, implement and test a deep learning architecture to detect and identify damage in images. Digitization allows to make historical pictures and art much more widely available to the public. Many such pictures have suffered some form of damage due to time, storage conditions and the fragility of the original medium. For example, the image below (A) shows an example of a digitized parchment that has suffered significant damage over time.\n","\n","**The aim of this project is for you to design, implement and evaluate a deep learning model to detect and identify damage present in images.**\n","\n","<table>\n","<tr>\n","<td>\n","<div>\n","<img src=\"./damage_data/image_path/cljmrkz5n341f07clcujw105j.png\" width=\"500\"/>\n","</div>\n","</td>\n","<td>\n","<div>\n","<img src=\"./damage_data/annotation_rgb_path/cljmrkz5n341f07clcujw105j.png\" width=\"500\"/>\n","</div>\n","</td>\n","</tr>\n","<td><center>(A) Image</center></td><td><center>(B) damage labels</center></td>\n","</table>\n","*(Note that the images will only show once you have downloaded the dataset)*\n","\n","\n","The image labels in this figure (B) identifies a smatter of peeling paint, a large stained area in the bottom left and a missing part on the top left. Each colour in those images corresponds to a different category of damage, including `fold`, `writing` or `burn marks`. You are provided with a dataset of a variety of damaged images, from Parchment to ceramic or wood painting, and detailed annotations of a range of damages.\n","\n","You are free to use any architecture you prefer, from what we have seen in class. You can decide to use unsupervised pre-training of only supervised end-to-end training - the approach you choose is your choice.\n","\n","### Hand-in date: Friday 15th of March before 4:30pm (on Moodle)\n","\n","### Steps & Hints\n","* First, look at the data. What are the different type of images (content), what type of material, what type of damage? How different are they? What type of transformations for your data augmentation do you think would be acceptable here?.\n","* Second, check the provided helper functions for loading the data and separate into training and test set and cross-validation.\n","* Design a network for the task. What output? What layers? How many? Do you want to use an Autoencoder for unsupervised pre-training?\n","* Choose a loss function for your network\n","* Select optimiser and training parameters (batch size, learning rate)\n","* Optimise your model, and tune hyperparameters (especially learning rate, momentum etc)\n","* Analyse the results on the test data. How to measure success? Which classes are recognised well, which are not? Is there confusion between some classes? Look at failure cases.\n","* If time allows, go back to drawing board and try a more complex, or better, model.\n","* Explain your thought process, justify your choices and discuss the results!\n","\n","### Submission\n","* submit ONE zip file on Moodle containing:\n","  * **your notebook**: use `File -> download .ipynb` to download the notebook file locally from colab.\n","  * **a PDF file** of your notebook's output as you see it: use `File -> print` to generate a PDF.\n","* your notebook must clearly contains separate cells for:\n","  * setting up your model and data loader\n","  * training your model from data\n","  * loading your pretrained model from github/gitlab/any other online storage you like!\n","  * testing your model on test data.\n","* The training cells must be disabled by a flag, such that when running *run all* on your notebook it does\n","  * load the data\n","  * load your model\n","  * apply the model to the test data\n","  * analyse and display the results and accuracy\n","* In addition provide markup cell:\n","  * containing your student number at the top\n","  * to describe and motivate your design choices: architecture, pre-processing, training regime\n","  * to analyse, describe and comment on your results\n","  * to provide some discussion on what you think are the limitations of your solution and what could be future work\n","\n","* **Note that you must put your trained model online so that your code can download it.**\n","\n","\n","### Assessment criteria\n","* In order to get a pass mark, you will need to demonstrate that you have designed and trained a deep NN to solve the problem, using sensible approach and reasonable efforts to tune hyper-parameters. You have analysed the results. It is NOT necessary to have any level of accuracy (a network that predicts poorly will always yield a pass mark if it is designed, tuned and analysed sensibly).\n","* In order to get a good mark, you will show good understanding of the approach and provide a working solution.\n","* in order to get a high mark, you will demonstrate a working approach of gradual improvement between different versions of your solution.\n","* bonus marks for attempting something original if well motivated - even if it does not yield increased performance.\n","* bonus marks for getting high performance, and some more points are to grab for getting the best performance in the class.\n","\n","### Notes\n","* You are provided code to isolate the test set and cross validation, make sure to keep the separation clean to ensure proper setting of all hyperparameters.\n","* I recommend to start with small models that can be easier to train to set a baseline performance before attempting more complex one.\n","* Be mindful of the time!"]},{"cell_type":"markdown","metadata":{"id":"ZQa07mdpd6A6"},"source":["## Housekeeping"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9799,"status":"ok","timestamp":1709081354359,"user":{"displayName":"Tianrun Zhao","userId":"05932382251349082346"},"user_tz":0},"id":"GtWZa-JPesz7","outputId":"802e1318-2484-45d8-c588-cd7cbbbb2407"},"outputs":[],"source":["# !pip install gdown pytorch_lightning"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":12950,"status":"ok","timestamp":1709081367306,"user":{"displayName":"Tianrun Zhao","userId":"05932382251349082346"},"user_tz":0},"id":"FK_rOEmQuwV3"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning.loggers import WandbLogger\n","from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n","\n","import os\n","import pandas as pd\n","import random\n","import PIL\n","from PIL import Image\n","PIL.Image.MAX_IMAGE_PIXELS = 243748701\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import gdown\n","import shutil\n","from tqdm import tqdm\n","DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"CU2Ky6meo0kt"},"source":["# Load dataset"]},{"cell_type":"markdown","metadata":{"id":"aLMiDRyUd6A8"},"source":["We then load the metadata in a dataframe for convenience"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1709081367307,"user":{"displayName":"Tianrun Zhao","userId":"05932382251349082346"},"user_tz":0},"id":"6nM_cNHrgsjY","outputId":"b79f5b5b-55ac-4043-dbd2-0163d3353016"},"outputs":[],"source":["# !pwd"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1811,"status":"ok","timestamp":1709081369116,"user":{"displayName":"Tianrun Zhao","userId":"05932382251349082346"},"user_tz":0},"id":"n_T3a3WHiL8P","outputId":"fc642827-aa4d-4f1d-bdac-106fa6293ef7"},"outputs":[],"source":["# !gdown 1v8aUId0-tTW3ln3O2BE4XajQeCToOEiS -O damages.zip"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"executionInfo":{"elapsed":4052,"status":"error","timestamp":1709081373166,"user":{"displayName":"Tianrun Zhao","userId":"05932382251349082346"},"user_tz":0},"id":"0S7fMugEd6A8","outputId":"0f612116-a011-4168-f710-85dfbc3e0b27"},"outputs":[],"source":["# set  that to wherever you want to store the data (eg, your Google Drive), choose a persistent location!\n","root_dir = '.'\n","data_dir = os.path.join(root_dir, \"damage_data\")\n","csv_path = os.path.join(data_dir, 'metadata.csv')\n","\n","try:\n","    df = pd.read_csv(csv_path)\n","\n","except:  # if the dataset has not been downloaded yet, do it.\n","    zip_path = os.path.join(root_dir, 'damages.zip')\n","    gdown.download(id='1v8aUId0-tTW3ln3O2BE4XajQeCToOEiS', output=zip_path)\n","    shutil.unpack_archive(zip_path, root_dir)\n","    df = pd.read_csv(csv_path)"]},{"cell_type":"markdown","metadata":{"id":"18VmATnob3G_"},"source":["This dataframe has the paths of where the dataset images and annotation labels are stored, plus classification labels."]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":238,"status":"ok","timestamp":1709060223007,"user":{"displayName":"Tianrun Zhao","userId":"05932382251349082346"},"user_tz":0},"id":"obKQZoPbd6A8","outputId":"a9f8b3b8-32db-483e-f30f-c531dd365dcf"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>material</th>\n","      <th>content</th>\n","      <th>image_path</th>\n","      <th>annotation_path</th>\n","      <th>annotation_rgb_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>cljmrkz5n341f07clcujw105j</td>\n","      <td>Parchment</td>\n","      <td>Artistic depiction</td>\n","      <td>./damage_data/image_path/cljmrkz5n341f07clcujw...</td>\n","      <td>./damage_data/annotation_path/cljmrkz5n341f07c...</td>\n","      <td>./damage_data/annotation_rgb_path/cljmrkz5n341...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>cljmrkz5n341n07clf1u410ed</td>\n","      <td>Parchment</td>\n","      <td>Artistic depiction</td>\n","      <td>./damage_data/image_path/cljmrkz5n341n07clf1u4...</td>\n","      <td>./damage_data/annotation_path/cljmrkz5n341n07c...</td>\n","      <td>./damage_data/annotation_rgb_path/cljmrkz5n341...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>cljmrkz5n341r07clhl93dpre</td>\n","      <td>Parchment</td>\n","      <td>Artistic depiction</td>\n","      <td>./damage_data/image_path/cljmrkz5n341r07clhl93...</td>\n","      <td>./damage_data/annotation_path/cljmrkz5n341r07c...</td>\n","      <td>./damage_data/annotation_rgb_path/cljmrkz5n341...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>cljmrkz5n341v07cl2gfhd6zj</td>\n","      <td>Parchment</td>\n","      <td>Artistic depiction</td>\n","      <td>./damage_data/image_path/cljmrkz5n341v07cl2gfh...</td>\n","      <td>./damage_data/annotation_path/cljmrkz5n341v07c...</td>\n","      <td>./damage_data/annotation_rgb_path/cljmrkz5n341...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>cljmrkz5n341z07cldbn01un3</td>\n","      <td>Parchment</td>\n","      <td>Artistic depiction</td>\n","      <td>./damage_data/image_path/cljmrkz5n341z07cldbn0...</td>\n","      <td>./damage_data/annotation_path/cljmrkz5n341z07c...</td>\n","      <td>./damage_data/annotation_rgb_path/cljmrkz5n341...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>390</th>\n","      <td>clnofow7i00n2076ubpfodf4d</td>\n","      <td>Wood</td>\n","      <td>Artistic depiction</td>\n","      <td>./damage_data/image_path/clnofow7i00n2076ubpfo...</td>\n","      <td>./damage_data/annotation_path/clnofow7i00n2076...</td>\n","      <td>./damage_data/annotation_rgb_path/clnofow7i00n...</td>\n","    </tr>\n","    <tr>\n","      <th>391</th>\n","      <td>clnrm7fvu092q07840tq9zs03</td>\n","      <td>Wood</td>\n","      <td>Artistic depiction</td>\n","      <td>./damage_data/image_path/clnrm7fvu092q07840tq9...</td>\n","      <td>./damage_data/annotation_path/clnrm7fvu092q078...</td>\n","      <td>./damage_data/annotation_rgb_path/clnrm7fvu092...</td>\n","    </tr>\n","    <tr>\n","      <th>392</th>\n","      <td>clnrm7fvu092r0784a6p2m5li</td>\n","      <td>Wood</td>\n","      <td>Artistic depiction</td>\n","      <td>./damage_data/image_path/clnrm7fvu092r0784a6p2...</td>\n","      <td>./damage_data/annotation_path/clnrm7fvu092r078...</td>\n","      <td>./damage_data/annotation_rgb_path/clnrm7fvu092...</td>\n","    </tr>\n","    <tr>\n","      <th>393</th>\n","      <td>clnrm7fvu092s0784tgwccewe</td>\n","      <td>Wood</td>\n","      <td>Artistic depiction</td>\n","      <td>./damage_data/image_path/clnrm7fvu092s0784tgwc...</td>\n","      <td>./damage_data/annotation_path/clnrm7fvu092s078...</td>\n","      <td>./damage_data/annotation_rgb_path/clnrm7fvu092...</td>\n","    </tr>\n","    <tr>\n","      <th>394</th>\n","      <td>clnrm7fvu092t0784ltljhiu0</td>\n","      <td>Wood</td>\n","      <td>Artistic depiction</td>\n","      <td>./damage_data/image_path/clnrm7fvu092t0784ltlj...</td>\n","      <td>./damage_data/annotation_path/clnrm7fvu092t078...</td>\n","      <td>./damage_data/annotation_rgb_path/clnrm7fvu092...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>395 rows × 6 columns</p>\n","</div>"],"text/plain":["                            id   material             content  \\\n","0    cljmrkz5n341f07clcujw105j  Parchment  Artistic depiction   \n","1    cljmrkz5n341n07clf1u410ed  Parchment  Artistic depiction   \n","2    cljmrkz5n341r07clhl93dpre  Parchment  Artistic depiction   \n","3    cljmrkz5n341v07cl2gfhd6zj  Parchment  Artistic depiction   \n","4    cljmrkz5n341z07cldbn01un3  Parchment  Artistic depiction   \n","..                         ...        ...                 ...   \n","390  clnofow7i00n2076ubpfodf4d       Wood  Artistic depiction   \n","391  clnrm7fvu092q07840tq9zs03       Wood  Artistic depiction   \n","392  clnrm7fvu092r0784a6p2m5li       Wood  Artistic depiction   \n","393  clnrm7fvu092s0784tgwccewe       Wood  Artistic depiction   \n","394  clnrm7fvu092t0784ltljhiu0       Wood  Artistic depiction   \n","\n","                                            image_path  \\\n","0    ./damage_data/image_path/cljmrkz5n341f07clcujw...   \n","1    ./damage_data/image_path/cljmrkz5n341n07clf1u4...   \n","2    ./damage_data/image_path/cljmrkz5n341r07clhl93...   \n","3    ./damage_data/image_path/cljmrkz5n341v07cl2gfh...   \n","4    ./damage_data/image_path/cljmrkz5n341z07cldbn0...   \n","..                                                 ...   \n","390  ./damage_data/image_path/clnofow7i00n2076ubpfo...   \n","391  ./damage_data/image_path/clnrm7fvu092q07840tq9...   \n","392  ./damage_data/image_path/clnrm7fvu092r0784a6p2...   \n","393  ./damage_data/image_path/clnrm7fvu092s0784tgwc...   \n","394  ./damage_data/image_path/clnrm7fvu092t0784ltlj...   \n","\n","                                       annotation_path  \\\n","0    ./damage_data/annotation_path/cljmrkz5n341f07c...   \n","1    ./damage_data/annotation_path/cljmrkz5n341n07c...   \n","2    ./damage_data/annotation_path/cljmrkz5n341r07c...   \n","3    ./damage_data/annotation_path/cljmrkz5n341v07c...   \n","4    ./damage_data/annotation_path/cljmrkz5n341z07c...   \n","..                                                 ...   \n","390  ./damage_data/annotation_path/clnofow7i00n2076...   \n","391  ./damage_data/annotation_path/clnrm7fvu092q078...   \n","392  ./damage_data/annotation_path/clnrm7fvu092r078...   \n","393  ./damage_data/annotation_path/clnrm7fvu092s078...   \n","394  ./damage_data/annotation_path/clnrm7fvu092t078...   \n","\n","                                   annotation_rgb_path  \n","0    ./damage_data/annotation_rgb_path/cljmrkz5n341...  \n","1    ./damage_data/annotation_rgb_path/cljmrkz5n341...  \n","2    ./damage_data/annotation_rgb_path/cljmrkz5n341...  \n","3    ./damage_data/annotation_rgb_path/cljmrkz5n341...  \n","4    ./damage_data/annotation_rgb_path/cljmrkz5n341...  \n","..                                                 ...  \n","390  ./damage_data/annotation_rgb_path/clnofow7i00n...  \n","391  ./damage_data/annotation_rgb_path/clnrm7fvu092...  \n","392  ./damage_data/annotation_rgb_path/clnrm7fvu092...  \n","393  ./damage_data/annotation_rgb_path/clnrm7fvu092...  \n","394  ./damage_data/annotation_rgb_path/clnrm7fvu092...  \n","\n","[395 rows x 6 columns]"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"0DMf6xlKd6A9"},"source":["The images in the dataset are categorised in terms of the type of `material`, meaning what was the original picture on, eg, Parchment, Glass or Textile."]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1709060232064,"user":{"displayName":"Tianrun Zhao","userId":"05932382251349082346"},"user_tz":0},"id":"HyWU2V3Dd6A9","outputId":"9432ae7e-2cfa-486e-f330-6f66577b0407"},"outputs":[{"data":{"text/plain":["array(['Parchment', 'Film emulsion', 'Glass', 'Paper', 'Tesserae',\n","       'Canvas', 'Textile', 'Ceramic', 'Wood'], dtype=object)"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["df['material'].unique()"]},{"cell_type":"markdown","metadata":{"id":"UZ12Mf2Ud6A9"},"source":["Moreover, images are also categorised in terms on the `content` of the image, meaning what is depicted: eg, Line art, geometric patterns, etc."]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":195,"status":"ok","timestamp":1709060247470,"user":{"displayName":"Tianrun Zhao","userId":"05932382251349082346"},"user_tz":0},"id":"d1SzCR0qd6A9","outputId":"2bcacba6-5a96-45d4-efb0-548dd1de1c6a"},"outputs":[{"data":{"text/plain":["array(['Artistic depiction', 'Line art', 'Photographic depiction',\n","       'Geometric patterns'], dtype=object)"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["df['content'].unique()"]},{"cell_type":"markdown","metadata":{"id":"K8rH3KykdMNc"},"source":["## Labels\n","Segmentation labels are saved as a PNG image, where each number from 1 to 15 corresponds to a damage class like Peel, Scratch etc; the Background class is set to 255, and the Clean class (no damage) is set to 0. We also provide code to convert these annotation values to RGB colours for nicer visualisation, but for training you should use the original annotations."]},{"cell_type":"code","execution_count":36,"metadata":{"id":"Td8pW0M6WLZO"},"outputs":[],"source":["name_color_mapping = {\n","    \"Material loss\": \"#1CE6FF\",\n","    \"Peel\": \"#FF34FF\",\n","    \"Dust\": \"#FF4A46\",\n","    \"Scratch\": \"#008941\",\n","    \"Hair\": \"#006FA6\",\n","    \"Dirt\": \"#A30059\",\n","    \"Fold\": \"#FFA500\",\n","    \"Writing\": \"#7A4900\",\n","    \"Cracks\": \"#0000A6\",\n","    \"Staining\": \"#63FFAC\",\n","    \"Stamp\": \"#004D43\",\n","    \"Sticker\": \"#8FB0FF\",\n","    \"Puncture\": \"#997D87\",\n","    \"Background\": \"#5A0007\",\n","    \"Burn marks\": \"#809693\",\n","    \"Lightleak\": \"#f6ff1b\",\n","}\n","\n","class_names = [ 'Material loss', 'Peel', 'Dust', 'Scratch',\n","                'Hair', 'Dirt', 'Fold', 'Writing', 'Cracks', 'Staining', 'Stamp',\n","                'Sticker', 'Puncture', 'Burn marks', 'Lightleak', 'Background']\n","\n","class_to_id = {class_name: idx+1 for idx, class_name in enumerate(class_names)}\n","class_to_id['Background'] = 255  # Set the Background ID to 255\n","\n","def hex_to_rgb(hex_color: str) -> tuple:\n","    hex_color = hex_color.lstrip('#')\n","    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n","\n","id_to_rgb = {class_to_id[class_name]: hex_to_rgb(color) for class_name, color in name_color_mapping.items()}\n","id_to_rgb[0] = (0,0,0)\n","\n","# Create id2label mapping: ID to class name\n","id2label = {idx: class_name for class_name, idx in class_to_id.items()}\n","\n","# Create label2id mapping: class name to ID, which is the same as class_to_id\n","label2id = class_to_id\n","\n","# Non-damaged pixels\n","id2label[0] = 'Clean'\n","label2id['Clean'] = 0"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":343},"executionInfo":{"elapsed":206,"status":"ok","timestamp":1709061021145,"user":{"displayName":"Tianrun Zhao","userId":"05932382251349082346"},"user_tz":0},"id":"SoOFtDuud6A-","outputId":"b8c157d6-1686-423a-c63d-c9d8420c3d52"},"outputs":[{"data":{"text/markdown":["#### Colour labels for each damage type\n","- <span style=\"color: #1CE6FF\">Material loss</span>.\n","- <span style=\"color: #FF34FF\">Peel</span>.\n","- <span style=\"color: #FF4A46\">Dust</span>.\n","- <span style=\"color: #008941\">Scratch</span>.\n","- <span style=\"color: #006FA6\">Hair</span>.\n","- <span style=\"color: #A30059\">Dirt</span>.\n","- <span style=\"color: #FFA500\">Fold</span>.\n","- <span style=\"color: #7A4900\">Writing</span>.\n","- <span style=\"color: #0000A6\">Cracks</span>.\n","- <span style=\"color: #63FFAC\">Staining</span>.\n","- <span style=\"color: #004D43\">Stamp</span>.\n","- <span style=\"color: #8FB0FF\">Sticker</span>.\n","- <span style=\"color: #997D87\">Puncture</span>.\n","- <span style=\"color: #809693\">Burn marks</span>.\n","- <span style=\"color: #f6ff1b\">Lightleak</span>.\n","- <span style=\"color: #5A0007\">Background</span>.\n"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["from IPython.display import Markdown\n","\n","legend='#### Colour labels for each damage type\\n'\n","for damage in class_names:\n","    legend += '- <span style=\"color: {color}\">{damage}</span>.\\n'.format(color=name_color_mapping[damage], damage=damage)\n","display(Markdown(legend))"]},{"cell_type":"markdown","metadata":{"id":"Oantk2PzcEFb"},"source":["## Create dataset splits\n","\n","Here is an example of how to split the dataset for Leave-one-out cross validation (LOOCV) based on material."]},{"cell_type":"code","execution_count":38,"metadata":{"id":"ay2LWSuLiDom"},"outputs":[],"source":["def create_leave_one_out_splits(df, criterion='material'):\n","\n","    grouped = df.groupby(criterion)\n","    content_splits = {name: group for name, group in grouped}\n","    unique_val = df[criterion].unique()\n","\n","    # Initialize a dictionary to hold the train and validation sets for each LOOCV iteration\n","    loocv_splits = {}\n","\n","    for value in unique_val:\n","        # Create the validation set\n","        val_set = content_splits[value]\n","\n","        # Create the training set\n","        train_set = pd.concat([content_splits[c] for c in unique_val if c != value])\n","\n","        # Add these to the loocv_splits dictionary\n","        loocv_splits[value] = {'train_set': train_set, 'val_set': val_set}\n","\n","    return loocv_splits\n"]},{"cell_type":"markdown","metadata":{"id":"5y5NruT7d6A-"},"source":["For this coursework, we will want to assess the generalisation of the method, so for that we will keep one type of material (`Canvas`) as test set, and only train on the remaining ones."]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":195,"status":"ok","timestamp":1709061024951,"user":{"displayName":"Tianrun Zhao","userId":"05932382251349082346"},"user_tz":0},"id":"KNf29AtXd6A-","outputId":"23b9f3c1-2f06-431e-9bee-573553de723b"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Parchment' 'Film emulsion' 'Glass' 'Paper' 'Tesserae' 'Textile'\n"," 'Ceramic' 'Wood']\n"]}],"source":["# split the dataset according to material type\n","full_splits = create_leave_one_out_splits(df, 'material')\n","\n","# use Canvas as test set\n","test_set = full_splits['Canvas']['val_set']\n","\n","# use the rest as training set\n","train_set = full_splits['Canvas']['train_set']\n","\n","# prepare a leave-one-out cross validation for the training set\n","loocv_splits = create_leave_one_out_splits(train_set, 'material')\n","\n","# identify the different type of image content\n","unique_material = train_set['material'].unique()\n","print(unique_material)\n"]},{"cell_type":"markdown","metadata":{"id":"sJRkv1lXcNFq"},"source":["To help you, here are some helper functions to help crop and process images."]},{"cell_type":"code","execution_count":40,"metadata":{"id":"m9bJdsQGd6A_"},"outputs":[],"source":["def random_square_crop_params(image, target_size):\n","    width, height = image.size\n","    min_edge = min(width, height)\n","\n","    # Conditionally set the range for random crop size\n","    lower_bound = min(min_edge, target_size)\n","    upper_bound = max(min_edge, target_size)\n","\n","    # Generate crop_size\n","    crop_size = random.randint(lower_bound, upper_bound)\n","\n","    # Check and adjust if crop_size is larger than any dimension of the image\n","    if crop_size > width or crop_size > height:\n","        crop_size = min(width, height)\n","\n","    # Generate random coordinates for the top-left corner of the crop\n","    x = random.randint(0, width - crop_size)\n","    y = random.randint(0, height - crop_size)\n","\n","    return (x, y, x + crop_size, y + crop_size)\n","\n","def apply_crop_and_resize(image, coords, target_size):\n","    image_crop = image.crop(coords)\n","    image_crop = image_crop.resize((target_size, target_size), Image.Resampling.NEAREST)\n","    return image_crop"]},{"cell_type":"markdown","metadata":{"id":"AOlkN-lVd6A_"},"source":["We also provide a simple class for holding the dataset"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"qBjlhUmnd6A_"},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","import random\n","import numpy as np\n","from PIL import Image\n","\n","from torchvision import transforms\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, dataframe, target_size, is_train=True):\n","        self.dataframe = dataframe\n","        self.target_size = target_size\n","        self.is_train = is_train\n","\n","        self.to_tensor = transforms.ToTensor()\n","\n","        # Define the normalization transform\n","        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                              std=[0.229, 0.224, 0.225])\n","\n","    def __len__(self):\n","            return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        row = self.dataframe.iloc[idx]\n","        image = Image.open(row['image_path']).convert('RGB')\n","        annotation = Image.open(row['annotation_path']).convert('L')\n","        annotation_rgb = Image.open(row['annotation_rgb_path']).convert('RGB')\n","        id = row['id']\n","        material = row['material']\n","        content = row['content']\n","\n","        if self.is_train:\n","            # Generate random square cropping coordinates\n","            crop_coords = random_square_crop_params(image, self.target_size)\n","\n","            # Apply the same cropping and resizing to all\n","            image = apply_crop_and_resize(image, crop_coords, self.target_size)\n","            annotation = apply_crop_and_resize(annotation, crop_coords, self.target_size)\n","            annotation_rgb = apply_crop_and_resize(annotation_rgb, crop_coords, self.target_size)\n","        else:  # Validation\n","            # Instead of cropping, downsize the images so that the longest edge is 1024 or less\n","            max_edge = max(image.size)\n","            if max_edge > 1024:\n","                downsample_ratio = 1024 / max_edge\n","                new_size = tuple([int(dim * downsample_ratio) for dim in image.size])\n","\n","                image = image.resize(new_size, Image.Resampling.BILINEAR)\n","                annotation = annotation.resize(new_size, Image.Resampling.NEAREST)\n","                annotation_rgb = annotation_rgb.resize(new_size, Image.Resampling.BILINEAR)\n","\n","        # Convert PIL images to PyTorch tensors\n","        image = self.to_tensor(image)\n","        annotation = torch.tensor(np.array(annotation), dtype=torch.long)\n","        annotation_rgb = self.to_tensor(annotation_rgb)\n","\n","        # Normalize the image\n","        image = self.normalize(image)\n","\n","        # Change all values in annotation that are 255 to 16\n","        annotation[annotation == 255] = 16\n","\n","        return {\n","            'image': image,\n","            'annotation': annotation,\n","            'annotation_rgb': annotation_rgb,\n","            'id': id,\n","            'material': material,\n","            'content': content\n","        }"]},{"cell_type":"markdown","metadata":{"id":"mvI-9NaxcljF"},"source":["Here we create a DataModule which encapsulates our training and validation DataLoaders; you can also do this manually by only using the Pytorch DataLoader class, lines 24 and 27."]},{"cell_type":"code","execution_count":42,"metadata":{"id":"SPuGFmPGuIFk"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","class CustomDataModule(pl.LightningDataModule):\n","    def __init__(self, loocv_splits, current_material, target_size, batch_size=32, num_workers=0):\n","        super().__init__()\n","        self.loocv_splits = loocv_splits\n","        self.current_material = current_material\n","        self.target_size = target_size\n","        self.batch_size = batch_size\n","        self.num_workers = num_workers\n","\n","    def prepare_data(self):\n","        pass\n","\n","    def setup(self, stage=None):\n","        # Load current train and validation set based on LOOCV iteration\n","        train_df = self.loocv_splits[self.current_material]['train_set']\n","        val_df = self.loocv_splits[self.current_material]['val_set'].sample(frac=1).reset_index(drop=True)\n","\n","        self.train_dataset = CustomDataset(dataframe=train_df, target_size=self.target_size, is_train=True)\n","        self.val_dataset = CustomDataset(dataframe=val_df, target_size=self.target_size, is_train=False)\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=1, shuffle=False, num_workers=self.num_workers)\n","\n","    def test_dataloader(self):\n","        pass\n"]},{"cell_type":"markdown","metadata":{"id":"KR1Qgc4Sd6A_"},"source":["The following will create a data module for validating on the first content in the list (`Parchment`) and training on all the other types of material (you will want to do that for each fold)."]},{"cell_type":"code","execution_count":43,"metadata":{"id":"MEIvJAAuwKBb"},"outputs":[],"source":["# data_module = CustomDataModule(loocv_splits=loocv_splits,\n","#                                current_material=unique_material[3], # 0  0-7\n","#                                target_size=512,\n","#                                batch_size=4)"]},{"cell_type":"markdown","metadata":{"id":"CkEbiloVd6BA"},"source":["Finally, we can get the train and validation data loaders from the data module."]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":292,"status":"ok","timestamp":1709061034142,"user":{"displayName":"Tianrun Zhao","userId":"05932382251349082346"},"user_tz":0},"id":"E2nVOsBIwZPq","outputId":"4400ba74-2dd6-4713-f391-f321a2f7daa3"},"outputs":[],"source":["# data_module.setup()\n","# train_loader = data_module.train_dataloader()\n","# val_loader = data_module.val_dataloader()"]},{"cell_type":"markdown","metadata":{"id":"MevlLaqfc-XA"},"source":["# Dataset visualisation"]},{"cell_type":"markdown","metadata":{"id":"xShAlBKvc32-"},"source":["We need to denormalise the images so we can display them"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"xHT-CU9tyblk"},"outputs":[],"source":["# Mean and std used for normalization\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","def denormalize(image, mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]):\n","    img_cpy = image.copy()\n","    for i in range(3):\n","        img_cpy[..., i] = img_cpy[..., i] * std[i] + mean[i]\n","    return img_cpy"]},{"cell_type":"markdown","metadata":{"id":"bw2zLzOJdBfb"},"source":["## Visualise training samples\n","Random square crops of the images and correspoding RGB annotations on their own and overlaid onto the image."]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1T8bTUq0drbBx-JYfEFX-xz3vGk56TYGJ"},"executionInfo":{"elapsed":12649,"status":"ok","timestamp":1709061052316,"user":{"displayName":"Tianrun Zhao","userId":"05932382251349082346"},"user_tz":0},"id":"zDlIgX7ZwdEr","outputId":"e5835259-41da-4051-e119-c6c2eeb1d1ab"},"outputs":[],"source":["# example_batch = next(iter(train_loader))\n","\n","# example_images = example_batch['image']\n","# example_annotations = example_batch['annotation']\n","# example_annotation_rgbs = example_batch['annotation_rgb']\n","\n","# # Number of examples to visualize\n","# N = min(4, len(example_images))\n","\n","# fig, axes = plt.subplots(N, 3, figsize=(15, 5 * N))\n","\n","# for ax, col in zip(axes[0], ['Image', 'Annotation', 'Overlay']):\n","#     ax.set_title(col, fontsize=24)\n","\n","# for i in range(N):\n","#     example_image = denormalize(example_images[i].numpy().transpose((1, 2, 0)), mean, std)  # C, H, W -> H, W, C\n","#     example_annotation = Image.fromarray(np.uint8(example_annotations[i].numpy()), 'L')\n","#     example_annotation_rgb = example_annotation_rgbs[i].numpy().transpose((1, 2, 0))  # C, H, W -> H, W, C\n","\n","#     # Create an alpha (transparency) channel where black pixels in annotation_rgb are fully transparent\n","#     alpha_channel = np.all(example_annotation_rgb == [0, 0, 0], axis=-1)\n","#     example_annotation_rgba = np.dstack((example_annotation_rgb, np.where(alpha_channel, 0, 1)))\n","\n","#     axes[i, 0].imshow(example_image)\n","#     axes[i, 0].axis('off')\n","\n","#     #axes[i, 1].imshow(example_annotation, cmap='gray', vmin=0, vmax=255)\n","#     axes[i, 1].imshow(example_annotation_rgb)\n","#     axes[i, 1].axis('off')\n","\n","#     axes[i, 2].imshow(example_image)\n","#     axes[i, 2].imshow(example_annotation_rgba)\n","#     axes[i, 2].axis('off')\n","\n","# plt.tight_layout()\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"t71C8ZeXdyWS"},"source":["Visualising the validation set, which loads the left-out class as whole images."]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Xip7qwzv6CZ6i_cWp4OfJmU9DrE7sJOp"},"executionInfo":{"elapsed":9735,"status":"ok","timestamp":1709061130461,"user":{"displayName":"Tianrun Zhao","userId":"05932382251349082346"},"user_tz":0},"id":"anaTYaT6RNvH","outputId":"77635575-a095-4841-b0ab-dc9faf2d258f"},"outputs":[],"source":["# val_iter = iter(val_loader)\n","# example_batches = [next(val_iter) for _ in range(4)]\n","\n","# # Initialize empty lists to collect different parts of each batch\n","# example_images = []\n","# example_annotations = []\n","# example_annotation_rgbs = []\n","# example_materials = []\n","# example_contents = []\n","\n","# # Populate the lists with the data from the 4 batches\n","# for batch in example_batches:\n","#     example_images.append(batch['image'].squeeze())\n","#     example_annotations.append(batch['annotation'].squeeze())\n","#     example_annotation_rgbs.append(batch['annotation_rgb'].squeeze())\n","#     example_materials.append(batch['material'][0])\n","#     example_contents.append(batch['content'][0])\n","\n","# # Number of examples to visualize\n","# N = min(4, len(example_images))\n","\n","# fig, axes = plt.subplots(N, 3, figsize=(15, 5 * N))\n","\n","# for ax, col in zip(axes[0], ['Image', 'Annotation', 'Overlay']):\n","#     ax.set_title(col, fontsize=24)\n","\n","# for i in range(N):\n","#     example_image = denormalize(example_images[i].numpy().transpose((1, 2, 0)), mean, std)  # C, H, W -> H, W, C\n","#     example_annotation = example_annotations[i].numpy()\n","#     example_annotation_rgb = example_annotation_rgbs[i].numpy().transpose((1, 2, 0))  # C, H, W -> H, W, C\n","#     example_material = example_materials[i]\n","#     example_content = example_contents[i]\n","#     # Create an alpha (transparency) channel where black pixels in annotation_rgb are fully transparent\n","#     alpha_channel = np.all(example_annotation_rgb == [0, 0, 0], axis=-1)\n","#     example_annotation_rgba = np.dstack((example_annotation_rgb, np.where(alpha_channel, 0, 1)))\n","#     axes[i, 0].imshow(example_image)\n","#     axes[i, 0].axis('off')\n","\n","#     axes[i, 1].imshow(example_annotation_rgb)\n","#     axes[i, 1].axis('off')\n","\n","#     axes[i, 2].imshow(example_image)\n","#     axes[i, 2].imshow(example_annotation_rgba)\n","#     axes[i, 2].axis('off')\n","\n","# plt.tight_layout()\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"aJ8iDqqod6BB"},"source":["# Evaluation\n","\n","For the final evaluation of the model, make sure to test performance on the left out category, `Canvas` to have a fair idea on how well the model generalises."]},{"cell_type":"code","execution_count":48,"metadata":{"id":"556T0aPid6BB"},"outputs":[],"source":["test_module = CustomDataModule(loocv_splits=full_splits,\n","                               current_material='Canvas',\n","                               target_size=512,\n","                               batch_size=4)\n","\n","test_module.setup()\n","\n","test_loader = test_module.val_dataloader()\n"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["def dice_coeff(input, target, reduce_batch_first = False, epsilon = 1e-6):\n","    # Average of Dice coefficient for all batches, or for a single mask\n","    sum_dim = (-1, -2) if input.dim() == 2 or not reduce_batch_first else (-1, -2, -3)\n","\n","    inter = 2 * (input * target).sum(dim=sum_dim)\n","    sets_sum = input.sum(dim=sum_dim) + target.sum(dim=sum_dim)\n","    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n","\n","    dice = (inter + epsilon) / (sets_sum + epsilon)\n","    return dice.mean()\n","\n","def multiclass_dice_coeff(input, target, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n","    return dice_coeff(input.flatten(0, 1), target.flatten(0, 1), reduce_batch_first, epsilon)"]},{"cell_type":"markdown","metadata":{},"source":["my work"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","\n","\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(),\n","            nn.Dropout(p=0.3),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(),\n","            nn.Dropout(p=0.3),\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","\n","class Down(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.down = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            DoubleConv(in_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.down(x)\n","\n","\n","class Up(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n","        self.conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, feature1, feature2):\n","        diffY = feature2.size()[2] - self.up(feature1).size()[2]\n","        diffX = feature2.size()[3] - self.up(feature1).size()[3]\n","\n","        feature1 = F.pad(self.up(feature1), [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n","        return self.conv(torch.cat([feature2, feature1], dim=1))\n","    \n","class UNet(nn.Module):\n","    def __init__(self, n_channels, n_classes, features=[32,64,128,256]):\n","        super(UNet, self).__init__()\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","\n","        self.inc = DoubleConv(n_channels, features[0])\n","\n","        self.down_layers = nn.ModuleList([\n","            Down(features[i], features[i]*2) for i in range(len(features))\n","        ])\n","\n","        self.up_layers = nn.ModuleList([\n","            Up(features[i]*2, features[i]) for i in reversed(range(len(features)))\n","        ])\n","\n","        self.conv = nn.Conv2d(features[0], n_classes, kernel_size=1)\n","\n","    def forward(self, x):\n","        x_down = []\n","        x_up = []\n","\n","        x1 = self.inc(x)\n","        x_down.append(x1)\n","        x_up.append(x1)\n","\n","        for down in self.down_layers:\n","            x = down(x_down[-1])\n","            x_down.append(x)\n","            x_up.append(x)\n","\n","        for idx, up in enumerate(self.up_layers):\n","            x = up(x_up[-1], x_down.pop(len(self.up_layers)-idx-1))\n","            x_up.append(x)\n","\n","        logits = self.conv(x)\n","        return logits\n","    \n","def dice_loss(input, target, multiclass: bool = False):\n","    # Dice loss (objective to minimize) between 0 and 1\n","    fn = multiclass_dice_coeff if multiclass else dice_coeff\n","    return 1 - fn(input, target, reduce_batch_first=True)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["def save_model_and_image(model, loss_list, material):\n","    plt.plot(loss_list)\n","    plt.title(f'Training Loss {material}')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.savefig(f'./loss/training_loss_{material}.png')  # 指定保存路径和文件名\n","\n","    plt.show()  # 显示图像\n","    \n","    torch.save(model, f'./pre_trained_model/{material}_model.pth')"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/83 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:53<00:00,  1.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/20], loss:3.472170352935791\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:52<00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2/20], loss:3.1858975887298584\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:52<00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3/20], loss:3.082426071166992\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:52<00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [4/20], loss:2.743643045425415\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:53<00:00,  1.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [5/20], loss:2.332481622695923\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:53<00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [6/20], loss:2.3156471252441406\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:53<00:00,  1.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [7/20], loss:2.215599536895752\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:53<00:00,  1.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [8/20], loss:1.751953125\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:52<00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [9/20], loss:1.777048110961914\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:52<00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [10/20], loss:1.4823572635650635\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:52<00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [11/20], loss:2.0452966690063477\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:52<00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [12/20], loss:1.7139333486557007\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:52<00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [13/20], loss:2.2116143703460693\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:52<00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [14/20], loss:1.5995885133743286\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:52<00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [15/20], loss:2.4277737140655518\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:52<00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch [16/20], loss:1.7180299758911133\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:52<00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [17/20], loss:0.7959420084953308\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:52<00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [18/20], loss:0.5687190890312195\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:52<00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [19/20], loss:1.0377246141433716\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 83/83 [00:52<00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [20/20], loss:0.6921262145042419\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 42/42 [00:05<00:00,  7.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["dice:  tensor(0.7946, device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":[" 39%|███▊      | 34/88 [00:21<00:34,  1.56it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[52], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m loss_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m     23\u001b[0m         image \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     24\u001b[0m         annotation \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n","File \u001b[1;32mc:\\Users\\hfcao\\.conda\\envs\\yolo\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\hfcao\\.conda\\envs\\yolo\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32mc:\\Users\\hfcao\\.conda\\envs\\yolo\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32mc:\\Users\\hfcao\\.conda\\envs\\yolo\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[1;32mc:\\Users\\hfcao\\.conda\\envs\\yolo\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[1;32mIn[41], line 27\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     26\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[1;32m---> 27\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     annotation \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation_path\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m     annotation_rgb \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation_rgb_path\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[1;32mc:\\Users\\hfcao\\.conda\\envs\\yolo\\lib\\site-packages\\PIL\\Image.py:921\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    875\u001b[0m ):\n\u001b[0;32m    876\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    923\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\hfcao\\.conda\\envs\\yolo\\lib\\site-packages\\PIL\\ImageFile.py:260\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    255\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m         )\n\u001b[0;32m    259\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 260\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for i, material in enumerate(unique_material):\n","    data_module = CustomDataModule(loocv_splits=loocv_splits,\n","                                current_material=unique_material[i], # 0  0-7\n","                                target_size=512,\n","                                batch_size=4)\n","\n","    data_module.setup()\n","    train_loader = data_module.train_dataloader()\n","    val_loader = data_module.val_dataloader()\n","\n","    output_channels = len(id_to_rgb)\n","    num_epochs = 20\n","\n","\n","    model = UNet(3,output_channels).to(DEVICE)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n","\n","    loss_list = []\n","    for epoch in range(num_epochs):\n","        for data in tqdm(train_loader):\n","            image = data['image'].to(DEVICE)\n","            annotation = data['annotation'].to(DEVICE)\n","            optimizer.zero_grad()\n","\n","            recon = model(image)\n","            loss = criterion(recon, annotation)\n","            loss += dice_loss(\n","                                F.softmax(recon, dim=1).float(),\n","                                F.one_hot(annotation, 17).permute(0, 3, 1, 2).float(),\n","                                multiclass=True\n","                            )\n","            loss.backward()\n","            optimizer.step()\n","        scheduler.step(loss)\n","        loss_list.append(loss.item())\n","        print(f'Epoch [{epoch+1}/{num_epochs}], loss:{loss.item()}')\n","\n","    model.eval()\n","    dice_score = 0\n","    # iterate over the validation set\n","    with torch.no_grad():\n","        for batch in tqdm(val_loader):\n","            image = batch['image'].to(DEVICE)\n","            mask_true = batch['annotation'].to(DEVICE)\n","\n","            # move images and labels to correct device and type\n","            image = image.to(device=DEVICE)\n","            mask_true = mask_true.to(device=DEVICE)\n","        \n","            # predict the mask\n","            mask_pred = model(image)\n","            \n","            # convert to one-hot format\n","            mask_true = F.one_hot(mask_true, 17).permute(0, 3, 1, 2).float()\n","            mask_pred = F.one_hot(mask_pred.argmax(dim=1), 17).permute(0, 3, 1, 2).float()\n","            # compute the Dice score, ignoring background\n","            dice_score += multiclass_dice_coeff(mask_pred[:, 1:], mask_true[:, 1:], reduce_batch_first=False)\n","\n","    print(\"dice: \", dice_score / max(len(val_loader), 1))\n","\n","    # save_model_and_image(model, loss_list, material)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model.eval()\n","# dice_score = 0\n","# # iterate over the validation set\n","# with torch.no_grad():\n","#     for batch in tqdm(test_loader):\n","#         image = batch['image'].to(DEVICE)\n","#         mask_true = batch['annotation'].to(DEVICE)\n","\n","#         # move images and labels to correct device and type\n","#         image = image.to(device=DEVICE)\n","#         mask_true = mask_true.to(device=DEVICE)\n","       \n","#         # predict the mask\n","#         mask_pred = model(image)\n","        \n","#         # convert to one-hot format\n","#         mask_true = F.one_hot(mask_true, 17).permute(0, 3, 1, 2).float()\n","#         mask_pred = F.one_hot(mask_pred.argmax(dim=1), 17).permute(0, 3, 1, 2).float()\n","#         # compute the Dice score, ignoring background\n","#         dice_score += multiclass_dice_coeff(mask_pred[:, 1:], mask_true[:, 1:], reduce_batch_first=False)\n","\n","# print(\"dice: \", dice_score / max(len(test_loader), 1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["        \n","\n","    # model.eval()\n","    # dice_score = 0\n","    # # iterate over the validation set\n","    # with torch.no_grad():\n","    #     for batch in tqdm(val_loader):\n","    #         image = batch['image'].to(DEVICE)\n","    #         mask_true = batch['annotation'].to(DEVICE)\n","\n","    #         # move images and labels to correct device and type\n","    #         image = image.to(device=DEVICE)\n","    #         mask_true = mask_true.to(device=DEVICE)\n","        \n","    #         # predict the mask\n","    #         mask_pred = model(image)\n","            \n","    #         # convert to one-hot format\n","    #         mask_true = F.one_hot(mask_true, 17).permute(0, 3, 1, 2).float()\n","    #         mask_pred = F.one_hot(mask_pred.argmax(dim=1), 17).permute(0, 3, 1, 2).float()\n","    #         # compute the Dice score, ignoring background\n","    #         dice_score += multiclass_dice_coeff(mask_pred[:, 1:], mask_true[:, 1:], reduce_batch_first=False)\n","\n","    # print(\"dice: \", dice_score / max(len(val_loader), 1))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1yBBjzQWpOgHFF4LNyQQYfnT8s98cVFYP","timestamp":1709060029829}],"toc_visible":true},"kernelspec":{"display_name":"test","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
